{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "# You need to run ICSD_entries script first so you have them in ENTRIES directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties(url, max_tries=15):\n",
    "    '''Returns json using the url'''\n",
    "    url += \"?format=json\"\n",
    "    \n",
    "    # setting number of tries when url does not respond \n",
    "    remaining_tries = max_tries\n",
    "    \n",
    "    while remaining_tries > 0:\n",
    "        try:\n",
    "            with urlopen(url) as response:\n",
    "                source = response.read()\n",
    "                \n",
    "            # convert string (source) to dict\n",
    "            data = json.loads(source)\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        except:\n",
    "            # waits 30 seconds until next retry\n",
    "            time.sleep(30)\n",
    "            print('Trying again.')\n",
    "            \n",
    "        remaining_tries = remaining_tries - 1\n",
    "        \n",
    "    # case after we try the url for 15 times with no response \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(prop, entry):\n",
    "    ''' Converts to DataFrame object'''\n",
    "        \n",
    "    try:\n",
    "        prop_df = pd.DataFrame(prop)\n",
    "        prop_df = prop_df.loc[0]\n",
    "    except:\n",
    "        # if we get any errors with the json to DataFrame conversion\n",
    "        # only a blank line with the compound's name is added\n",
    "        blank_line = {'compound': [entry]}\n",
    "        return pd.DataFrame(blank_line)\n",
    "    \n",
    "    # transposition operation needed for propper formatting\n",
    "    prop_df = prop_df.transpose()\n",
    "    \n",
    "    return prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, group):\n",
    "    ''' Saves the group's DataFrame df in PROPERTIES directory\n",
    "    in csv format.'''\n",
    "    \n",
    "    folder = './PROPERTIES'\n",
    "    # Making sure if PROPERTIES directory exists. If not, creates it.\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    # Saving\n",
    "    df.to_csv(f'{folder}/{group}_prop.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_group_prop(group):\n",
    "    '''Gets properties data for the group and saves it as group_prop.csv.\n",
    "    Returns list with missing entries'''\n",
    "    \n",
    "    # getting the entries from the group files\n",
    "    # these files were created with ICSD_entries.py\n",
    "    with open(f'./ENTRIES/{group}.csv') as f:\n",
    "        dados = f.read()\n",
    "\n",
    "    # each line --> one element in 'lista'\n",
    "    lista = dados.split('\\n')\n",
    "    # deleting first blank line/element\n",
    "    del lista[0]\n",
    "    \n",
    "    # Creating empty DataFrame for future appending (for each entry)\n",
    "    group_df = pd.DataFrame()\n",
    "    \n",
    "    # Printing just for logging\n",
    "    print(f\"{group} >> Getting {group} properties from ICSD_WEB.\\n\")\n",
    "    t_i = time.time()\n",
    "    \n",
    "    # Iterating thorugh the list of entries\n",
    "    for entry in lista:\n",
    "        \n",
    "        # adding entry to the url\n",
    "        url = f'http://aflowlib.duke.edu/AFLOWDATA/ICSD_WEB/{group}/{entry}'\n",
    "        \n",
    "        properties = get_properties(url)\n",
    "        \n",
    "        # Empty list for appending of missing entries\n",
    "        missing_entries = list()\n",
    "        \n",
    "        # if properties == None: get the index and the entry\n",
    "        # for future verification\n",
    "        if not properties:\n",
    "            index_missing, entry_missing  = lista.index(entry), entry\n",
    "            missing_entries.append((index_missing, entry_missing))\n",
    "            print(f'{group} >> MISSED ENTRY: {index_missing}, {entry_missing}')\n",
    "            continue\n",
    "              \n",
    "        # Converting to json to Dataframe structure\n",
    "        prop_df = convert_to_df(properties, entry)\n",
    "        \n",
    "        # Appending to the group DataFrame\n",
    "        group_df = group_df.append(prop_df, sort='False')\n",
    "        \n",
    "        pid = os.getpid()\n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'Process {pid} > {st} - {url[44:]}\\n{group} > SIZE: {group_df.shape}')\n",
    "        \n",
    "    save_to_csv(group_df, group)\n",
    "    \n",
    "    # Logging        \n",
    "    delta = round((time.time() - t_i)/60, 2)\n",
    "    print(f\"{group} >> Done. It took {delta} minutes.\\n\")\n",
    "    \n",
    "    # Returning tuple info about missing entries\n",
    "    return (group, missing_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    server = \"http://aflowlib.duke.edu/AFLOWDATA/ICSD_WEB\"\n",
    "    group_list = \"BCC BCT CUB FCC HEX MCL MCLC ORC ORCC ORCF ORCI RHL TET TRI\".split()\n",
    "    \n",
    "    print('>>> GETTING ALL DATA FROM ICSD_WEB <<<\\n\\n')\n",
    "    \n",
    "    t_i = time.time()\n",
    "    \n",
    "    # Parallelization stuff to get things faster \n",
    "    # you should test for the optimal number of threads\n",
    "    pool = ThreadPool(len(group_list))\n",
    "    \n",
    "    # This line starts everything\n",
    "    # pool.map({function you want to apply}, {list of objects used as args})\n",
    "    # returns a list of outputs corresponding to the list of inputs\n",
    "    missed_entries = pool.map(save_group_prop, group_list)\n",
    "    \n",
    "    delta = round((time.time() - t_i)/60, 2)\n",
    "    print(f'\\n\\n >>> FINISHED. IT TOOK {delta} minutes. <<<\\n')   \n",
    "    print(missed_entries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
